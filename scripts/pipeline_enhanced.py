#!/usr/bin/env python3
"""
Video Expert Analyzer Pipeline - Enhanced Version
æ”¯æŒï¼šé…ç½®ç›®å½•é€‰æ‹©ã€ç²¾é€‰ç‰‡æ®µå­æ–‡ä»¶å¤¹ã€è¯¦ç»†åˆ†ææŠ¥å‘Š
"""

import os
import sys
import json
import argparse
import subprocess
import tempfile
import re
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Optional, Tuple


# é…ç½®æ–‡ä»¶è·¯å¾„
CONFIG_DIR = Path.home() / ".config" / "video-expert-analyzer"
CONFIG_FILE = CONFIG_DIR / "config.json"


def load_config() -> Dict:
    """åŠ è½½ç”¨æˆ·é…ç½®ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™åˆ›å»ºé»˜è®¤é…ç½®"""
    CONFIG_DIR.mkdir(parents=True, exist_ok=True)
    
    if CONFIG_FILE.exists():
        with open(CONFIG_FILE, 'r', encoding='utf-8') as f:
            return json.load(f)
    
    # é»˜è®¤é…ç½®
    default_config = {
        "output_base_dir": str(Path.home() / "Downloads" / "video-analysis"),
        "first_run": True,
        "default_whisper_model": "base",
        "default_scene_threshold": 27.0
    }
    
    save_config(default_config)
    return default_config


def save_config(config: Dict):
    """ä¿å­˜ç”¨æˆ·é…ç½®"""
    CONFIG_DIR.mkdir(parents=True, exist_ok=True)
    with open(CONFIG_FILE, 'w', encoding='utf-8') as f:
        json.dump(config, f, indent=2, ensure_ascii=False)


def get_video_info(url: str) -> Dict:
    """
    ä½¿ç”¨ yt-dlp è·å–è§†é¢‘ä¿¡æ¯ï¼ˆæ ‡é¢˜ã€ä½œè€…ç­‰ï¼‰
    """
    try:
        cmd = [
            "yt-dlp",
            "--print", "%(title)s",
            "--print", "%(uploader)s",
            "--print", "%(channel)s",
            "--print", "%(duration)s",
            "--print", "%(view_count)s",
            "--no-download",
            url
        ]
        result = subprocess.run(cmd, capture_output=True, text=True, timeout=30)
        
        if result.returncode == 0:
            lines = result.stdout.strip().split('\n')
            return {
                "title": lines[0] if len(lines) > 0 else "",
                "uploader": lines[1] if len(lines) > 1 else "",
                "channel": lines[2] if len(lines) > 2 else "",
                "duration": lines[3] if len(lines) > 3 else "",
                "view_count": lines[4] if len(lines) > 4 else "",
                "success": True
            }
    except Exception as e:
        print(f"   âš ï¸  è·å–è§†é¢‘ä¿¡æ¯å¤±è´¥: {e}")
    
    return {"success": False, "title": "", "uploader": ""}


def sanitize_filename(name: str, max_length: int = 50) -> str:
    """
    æ¸…ç†æ–‡ä»¶åï¼Œç§»é™¤éæ³•å­—ç¬¦å¹¶é™åˆ¶é•¿åº¦
    """
    # ç§»é™¤éæ³•å­—ç¬¦
    name = re.sub(r'[\\/*?:"<>|]', '', name)
    # ç§»é™¤å¤šä½™ç©ºæ ¼
    name = re.sub(r'\s+', ' ', name).strip()
    
    # å¦‚æœè¶…é•¿ï¼Œæˆªå–å‰max_lengthä¸ªå­—ç¬¦
    if len(name) > max_length:
        name = name[:max_length].strip()
    
    return name


def generate_folder_name(video_info: Dict, video_id: str, max_length: int = 60) -> str:
    """
    ç”Ÿæˆè§†é¢‘æ–‡ä»¶å¤¹åç§°
    æ ¼å¼: [ä½œè€…] - [æ ‡é¢˜] æˆ– [æ ‡é¢˜]
    å¦‚æœè¶…é•¿åˆ™æå–å…³é”®å­—
    """
    title = video_info.get("title", "")
    uploader = video_info.get("uploader", "") or video_info.get("channel", "")
    
    if not title:
        return video_id
    
    # æ¸…ç†æ ‡é¢˜
    title = sanitize_filename(title, max_length=100)
    uploader = sanitize_filename(uploader, max_length=30)
    
    # ç”Ÿæˆæ–‡ä»¶å¤¹å
    if uploader:
        folder_name = f"[{uploader}] {title}"
    else:
        folder_name = title
    
    # å¦‚æœè¶…é•¿ï¼Œä½¿ç”¨ä½œè€…+ç®€å†™æ ‡é¢˜
    if len(folder_name) > max_length:
        # æå–æ ‡é¢˜å‰30ä¸ªå­—ç¬¦
        short_title = title[:30].strip()
        if uploader:
            folder_name = f"[{uploader}] {short_title}"
        else:
            folder_name = short_title
    
    # æœ€ç»ˆæ¸…ç†
    folder_name = folder_name.strip()
    if not folder_name:
        folder_name = video_id
    
    return folder_name


def setup_output_directory() -> str:
    """äº¤äº’å¼è®¾ç½®è¾“å‡ºç›®å½•"""
    config = load_config()
    
    print("=" * 60)
    print("ğŸ“ è¾“å‡ºç›®å½•é…ç½®")
    print("=" * 60)
    
    if config.get("first_run", True):
        print("\nğŸ‰ æ¬¢è¿ä½¿ç”¨ Video Expert Analyzer!")
        print("è¯·è®¾ç½®è§†é¢‘åˆ†æå’Œè¾“å‡ºçš„é»˜è®¤ç›®å½•\n")
    else:
        print(f"\nå½“å‰é»˜è®¤è¾“å‡ºç›®å½•: {config['output_base_dir']}")
    
    print("\né€‰é¡¹:")
    print("  1. ä½¿ç”¨å½“å‰ç›®å½•")
    print("  2. ä½¿ç”¨é»˜è®¤ç›®å½• (~/Downloads/video-analysis)")
    print("  3. è‡ªå®šä¹‰ç›®å½•")
    
    if not config.get("first_run", True):
        print("  4. ä¿®æ”¹å½“å‰é»˜è®¤ç›®å½•")
    
    try:
        choice = input("\nè¯·é€‰æ‹© [1-4]: ").strip()
    except (EOFError, KeyboardInterrupt):
        return config['output_base_dir']
    
    if choice == "1":
        output_dir = config['output_base_dir']
    elif choice == "2":
        output_dir = str(Path.home() / "Downloads" / "video-analysis")
        config['output_base_dir'] = output_dir
        save_config(config)
    elif choice == "3":
        try:
            custom_dir = input("è¯·è¾“å…¥è‡ªå®šä¹‰ç›®å½•è·¯å¾„: ").strip()
            output_dir = custom_dir
            config['output_base_dir'] = output_dir
            config['first_run'] = False
            save_config(config)
        except (EOFError, KeyboardInterrupt):
            output_dir = config['output_base_dir']
    elif choice == "4" and not config.get("first_run", True):
        try:
            new_dir = input("è¯·è¾“å…¥æ–°çš„é»˜è®¤ç›®å½•è·¯å¾„: ").strip()
            config['output_base_dir'] = new_dir
            save_config(config)
            output_dir = new_dir
        except (EOFError, KeyboardInterrupt):
            output_dir = config['output_base_dir']
    else:
        output_dir = config['output_base_dir']
    
    if config.get("first_run", True):
        config['first_run'] = False
        save_config(config)
    
    print(f"\nâœ… è¾“å‡ºç›®å½•: {output_dir}")
    return output_dir


def get_output_directory() -> str:
    """è·å–å½“å‰é…ç½®çš„è¾“å‡ºç›®å½•ï¼ˆéäº¤äº’å¼ï¼‰"""
    config = load_config()
    return config['output_base_dir']


class VideoAnalysisPipeline:
    """å¢å¼ºç‰ˆè§†é¢‘åˆ†æç®¡é“"""

    def __init__(self, url: str, output_dir: str,
                 whisper_model: str = "medium",
                 scene_threshold: float = 27.0,
                 extract_scenes: bool = True,
                 auto_select_best: bool = True,
                 best_threshold: float = 7.5):
        self.url = url
        self.output_dir = Path(output_dir).resolve()
        self.whisper_model = whisper_model
        self.scene_threshold = scene_threshold
        self.extract_scenes = extract_scenes
        self.auto_select_best = auto_select_best
        self.best_threshold = best_threshold

        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # æå–è§†é¢‘ ID
        self.video_id = self._extract_video_id(url)
        
        # è·å–è§†é¢‘ä¿¡æ¯ï¼ˆæ ‡é¢˜ã€ä½œè€…ç­‰ï¼‰
        print("\nğŸ“‹ æ­£åœ¨è·å–è§†é¢‘ä¿¡æ¯...")
        self.video_info = get_video_info(url)
        
        if self.video_info.get("success"):
            print(f"   æ ‡é¢˜: {self.video_info.get('title', 'N/A')[:60]}...")
            print(f"   ä½œè€…: {self.video_info.get('uploader', 'N/A')}")
            
            # ç”Ÿæˆä»¥æ ‡é¢˜å‘½åçš„æ–‡ä»¶å¤¹å
            self.folder_name = generate_folder_name(self.video_info, self.video_id)
            print(f"   æ–‡ä»¶å¤¹: {self.folder_name}")
        else:
            print("   âš ï¸  æ— æ³•è·å–è§†é¢‘ä¿¡æ¯ï¼Œä½¿ç”¨è§†é¢‘ ID ä½œä¸ºæ–‡ä»¶å¤¹å")
            self.folder_name = self.video_id
            self.video_info = {"title": self.video_id, "uploader": ""}
        
        # åˆ›å»ºè§†é¢‘ä¸“å±å­ç›®å½•ï¼ˆä½¿ç”¨æ ‡é¢˜å‘½åï¼‰
        self.video_output_dir = self.output_dir / self.folder_name
        self.video_output_dir.mkdir(parents=True, exist_ok=True)

        # Define output paths
        self.video_path = self.video_output_dir / f"{self.video_id}.mp4"
        self.audio_path = self.video_output_dir / f"{self.video_id}.m4a"
        self.srt_path = self.video_output_dir / f"{self.video_id}.srt"
        self.transcript_path = self.video_output_dir / f"{self.video_id}_transcript.txt"
        self.scenes_dir = self.video_output_dir / "scenes"
        self.best_shots_dir = self.scenes_dir / "best_shots"
        self.frames_dir = self.video_output_dir / "frames"
        self.scores_path = self.video_output_dir / "scene_scores.json"
        self.report_path = self.video_output_dir / f"{self.video_id}_analysis_report.md"
        self.detailed_report_path = self.video_output_dir / f"{self.video_id}_detailed_analysis.md"

        # Results tracking
        self.results = {
            "video_id": self.video_id,
            "video_title": self.video_info.get("title", ""),
            "video_uploader": self.video_info.get("uploader", ""),
            "folder_name": self.folder_name,
            "url": url,
            "timestamp": datetime.now().isoformat(),
            "status": "initialized",
            "steps_completed": [],
            "scene_analysis": [],
            "overall_assessment": {}
        }

    def _extract_video_id(self, url: str) -> str:
        """Extract video ID from URL"""
        if "bilibili.com" in url:
            if "/video/" in url:
                parts = url.split("/video/")[1].split("/")[0].split("?")[0]
                return parts
        if "youtube.com" in url or "youtu.be" in url:
            if "v=" in url:
                return url.split("v=")[1].split("&")[0]
            elif "youtu.be/" in url:
                return url.split("youtu.be/")[1].split("?")[0]
        return f"video_{int(datetime.now().timestamp())}"

    def run(self) -> Dict:
        """Execute the complete pipeline"""
        print("=" * 60)
        print("ğŸ¬ VIDEO EXPERT ANALYZER PIPELINE")
        print("=" * 60)
        print(f"ğŸ“º è§†é¢‘æ ‡é¢˜: {self.video_info.get('title', 'N/A')[:50]}...")
        print(f"ğŸ‘¤ è§†é¢‘ä½œè€…: {self.video_info.get('uploader', 'N/A')}")
        print(f"ğŸ”— Video URL: {self.url}")
        print(f"ğŸ“ Output Dir: {self.video_output_dir}")
        print(f"ğŸ†” Video ID: {self.video_id}")
        print("=" * 60)

        try:
            self._step_download_video()
            self._step_download_audio()
            scene_info = self._step_scene_detection()
            transcript_info = self._step_transcription()
            self._step_extract_frames(scene_info)
            self._step_prepare_scoring(scene_info)
            self._step_ai_scene_analysis(scene_info)
            if self.auto_select_best:
                self._step_auto_select_best_shots(scene_info)
            self._step_generate_detailed_report(scene_info, transcript_info)

            self.results["status"] = "completed"
            print("\n" + "=" * 60)
            print("âœ… PIPELINE COMPLETED SUCCESSFULLY")
            print("=" * 60)
            print(f"\nğŸ“ æ‰€æœ‰æ–‡ä»¶ä¿å­˜åœ¨: {self.video_output_dir}")
            print(f"ğŸ“„ è¯¦ç»†åˆ†ææŠ¥å‘Š: {self.detailed_report_path}")
            return self.results

        except Exception as e:
            self.results["status"] = "failed"
            self.results["error"] = str(e)
            print(f"\nâŒ PIPELINE FAILED: {e}")
            raise

    def _step_download_video(self):
        print("\nğŸ“¥ Step 1: Downloading video...")
        if self.video_path.exists():
            print(f"   âš ï¸  Video already exists: {self.video_path}")
            self.results["steps_completed"].append("download_video")
            return
        cmd = ["yt-dlp", "-f", "bestvideo[ext=mp4]+bestaudio[ext=m4a]/best[ext=mp4]/best", "-o", str(self.video_path), self.url]
        subprocess.run(cmd, check=True)
        if not self.video_path.exists():
            raise RuntimeError("Video download failed - file not found")
        file_size = self.video_path.stat().st_size / (1024 * 1024)
        print(f"   âœ… Video downloaded: {file_size:.2f} MB")
        self.results["video_path"] = str(self.video_path)
        self.results["video_size_mb"] = round(file_size, 2)
        self.results["steps_completed"].append("download_video")

    def _step_download_audio(self):
        print("\nğŸµ Step 2: Downloading audio...")
        if self.audio_path.exists():
            print(f"   âš ï¸  Audio already exists: {self.audio_path}")
            self.results["steps_completed"].append("download_audio")
            return
        
        # é¦–å…ˆå°è¯•ä» URL ä¸‹è½½éŸ³é¢‘
        try:
            cmd = ["yt-dlp", "-f", "bestaudio[ext=m4a]/bestaudio", "--extract-audio", "--audio-format", "m4a", "-o", str(self.audio_path), self.url]
            subprocess.run(cmd, check=True, capture_output=True)
            if self.audio_path.exists():
                file_size = self.audio_path.stat().st_size / 1024
                print(f"   âœ… Audio downloaded: {file_size:.2f} KB")
                self.results["audio_path"] = str(self.audio_path)
                self.results["steps_completed"].append("download_audio")
                return
        except subprocess.CalledProcessError:
            print("   âš ï¸  Audio download failed, extracting from video...")
        
        # å¦‚æœä¸‹è½½å¤±è´¥ï¼Œä»å·²ä¸‹è½½çš„è§†é¢‘ä¸­æå–éŸ³é¢‘
        if self.video_path.exists():
            try:
                cmd = ["ffmpeg", "-i", str(self.video_path), "-vn", "-c:a", "copy", str(self.audio_path), "-y"]
                subprocess.run(cmd, check=True, capture_output=True)
                if self.audio_path.exists():
                    file_size = self.audio_path.stat().st_size / 1024
                    print(f"   âœ… Audio extracted from video: {file_size:.2f} KB")
                    self.results["audio_path"] = str(self.audio_path)
                    self.results["steps_completed"].append("download_audio")
                    return
            except subprocess.CalledProcessError as e:
                print(f"   âš ï¸  Audio extraction failed: {e}")
        
        # å¦‚æœéƒ½å¤±è´¥äº†ï¼Œä½†è§†é¢‘å­˜åœ¨ï¼Œç»§ç»­å¤„ç†ï¼ˆä½¿ç”¨è§†é¢‘è¿›è¡Œè½¬å½•ï¼‰
        if self.video_path.exists():
            print("   âš ï¸  Will use video directly for transcription")
            self.results["steps_completed"].append("download_audio")
            return
            
        raise RuntimeError("Audio download/extraction failed - file not found")

    def _step_scene_detection(self) -> Dict:
        print("\nğŸï¸  Step 3: Detecting scenes...")
        self.scenes_dir.mkdir(exist_ok=True)
        cmd = ["scenedetect", "-i", str(self.video_path), "-o", str(self.scenes_dir), "detect-adaptive", "-t", str(self.scene_threshold)]
        if self.extract_scenes:
            cmd.append("split-video")
        result = subprocess.run(cmd, capture_output=True, text=True)
        output = result.stdout + result.stderr
        scene_count = 0
        for line in output.split("\n"):
            if "Detected" in line and "scenes" in line:
                parts = line.split()
                for i, part in enumerate(parts):
                    if part == "Detected" and i + 1 < len(parts):
                        try:
                            scene_count = int(parts[i + 1])
                            break
                        except ValueError:
                            pass
        if self.extract_scenes:
            scene_files = sorted(self.scenes_dir.glob("*.mp4"))
            scene_count = len(scene_files)
        print(f"   âœ… Detected {scene_count} scenes")
        scene_info = {"scene_count": scene_count, "scenes_dir": str(self.scenes_dir) if self.extract_scenes else None, "threshold": self.scene_threshold}
        self.results["scene_detection"] = scene_info
        self.results["steps_completed"].append("scene_detection")
        return scene_info

    def _step_transcription(self) -> Dict:
        print("\nğŸ¤ Step 4: Transcribing audio...")
        if self.srt_path.exists():
            print(f"   âš ï¸  Transcription already exists: {self.srt_path}")
            self.results["steps_completed"].append("transcription")
            return {"status": "skipped"}
        
        # ç¡®å®šéŸ³é¢‘æºï¼šä¼˜å…ˆä½¿ç”¨éŸ³é¢‘æ–‡ä»¶ï¼Œå¦åˆ™ä½¿ç”¨è§†é¢‘æ–‡ä»¶
        audio_source = str(self.audio_path) if self.audio_path.exists() else str(self.video_path)
        if audio_source == str(self.video_path):
            print(f"   âš ï¸  Using video file for transcription: {self.video_path.name}")
        
        import whisper
        import torch
        device = "cuda" if torch.cuda.is_available() else "cpu"
        if device == "cuda":
            print(f"   ğŸš€ Using CUDA acceleration")
        else:
            print(f"   âš ï¸  Using CPU (slower)")
        print(f"   ğŸ“¥ Loading Whisper {self.whisper_model} model...")
        model = whisper.load_model(self.whisper_model, device=device)
        with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as tmp:
            wav_path = tmp.name
        try:
            cmd = ["ffmpeg", "-y", "-i", audio_source, "-vn", "-acodec", "pcm_s16le", "-ar", "16000", "-ac", "1", wav_path]
            subprocess.run(cmd, check=True, capture_output=True)
            print(f"   ğŸ¤ Transcribing...")
            result = model.transcribe(wav_path, task="transcribe")
            self._write_srt(result["segments"], self.srt_path)
            self._write_transcript(result["segments"], self.transcript_path)
            print(f"   âœ… Transcription complete")
            print(f"      Language: {result.get('language', 'unknown')}")
            print(f"      Segments: {len(result['segments'])}")
            transcript_info = {"language": result.get("language", "unknown"), "segment_count": len(result["segments"]), "srt_path": str(self.srt_path), "transcript_path": str(self.transcript_path), "full_text": " ".join([seg["text"].strip() for seg in result["segments"]])}
            self.results["transcription"] = transcript_info
            self.results["steps_completed"].append("transcription")
            return transcript_info
        finally:
            if os.path.exists(wav_path):
                os.unlink(wav_path)

    def _write_srt(self, segments: List[Dict], output_path: Path):
        with open(output_path, "w", encoding="utf-8") as f:
            for i, segment in enumerate(segments, 1):
                start = self._format_timestamp(segment["start"])
                end = self._format_timestamp(segment["end"])
                text = segment["text"].strip()
                f.write(f"{i}\\n{start} --> {end}\\n{text}\\n\\n")

    def _write_transcript(self, segments: List[Dict], output_path: Path):
        with open(output_path, "w", encoding="utf-8") as f:
            f.write("=== Video Transcript ===\\n\\n")
            f.write("=== Full Text ===\\n\\n")
            full_text = " ".join([seg["text"].strip() for seg in segments])
            f.write(full_text + "\\n\\n")
            f.write("=== Timestamped Text ===\\n\\n")
            for seg in segments:
                start = self._format_timestamp(seg["start"])
                end = self._format_timestamp(seg["end"])
                f.write(f"[{start} --> {end}]\\n")
                f.write(f"{seg['text'].strip()}\\n\\n")

    def _format_timestamp(self, seconds: float) -> str:
        hours = int(seconds // 3600)
        minutes = int((seconds % 3600) // 60)
        secs = int(seconds % 60)
        millis = int((seconds % 1) * 1000)
        return f"{hours:02d}:{minutes:02d}:{secs:02d},{millis:03d}"

    def _step_extract_frames(self, scene_info: Dict):
        print("\\nğŸ–¼ï¸  Step 5: Extracting frames from scenes...")
        if not self.extract_scenes or scene_info["scene_count"] == 0:
            print("   âš ï¸  Scene extraction disabled or no scenes found")
            return
        self.frames_dir.mkdir(exist_ok=True)
        scene_files = sorted(self.scenes_dir.glob("*.mp4"))
        for scene_file in scene_files:
            scene_name = scene_file.stem
            # ä½¿ç”¨ sanitize_filename å¤„ç†åœºæ™¯æ–‡ä»¶åï¼Œé¿å…ç‰¹æ®Šå­—ç¬¦å¯¼è‡´ ffmpeg é”™è¯¯
            safe_scene_name = sanitize_filename(scene_name)
            frame_path = self.frames_dir / f"{safe_scene_name}.jpg"
            if frame_path.exists():
                continue
            cmd = ["ffmpeg", "-i", str(scene_file), "-vf", "select=eq(n\\\\,0)", "-vframes", "1", str(frame_path), "-y"]
            subprocess.run(cmd, check=True, capture_output=True)
        frame_count = len(list(self.frames_dir.glob("*.jpg")))
        print(f"   âœ… Extracted {frame_count} frames")
        self.results["frames_dir"] = str(self.frames_dir)
        self.results["frame_count"] = frame_count
        self.results["steps_completed"].append("extract_frames")

    def _step_prepare_scoring(self, scene_info: Dict):
        print("\\nğŸ“Š Step 6: Preparing scene scoring structure...")
        if not self.extract_scenes or scene_info["scene_count"] == 0:
            print("   âš ï¸  No scenes to score")
            return
        scene_files = sorted(self.scenes_dir.glob("*.mp4"))
        scoring_data = {
            "video_id": self.video_id,
            "url": self.url,
            "total_scenes": len(scene_files),
            "analysis_framework": {
                "philosophy": "Walter Murch's Six Rules: Emotion > Story > Rhythm > Eye-trace > 2D Plane > 3D Space",
                "scoring_criteria": {
                    "aesthetic_beauty": {"name": "ç¾æ„Ÿ (Aesthetic Beauty)", "weight": "20%", "description": "æ„å›¾(ä¸‰åˆ†æ³•)ã€å…‰å½±è´¨æ„Ÿã€è‰²å½©å’Œè°åº¦", "scale": "1-10"},
                    "credibility": {"name": "å¯ä¿¡åº¦ (Credibility)", "weight": "20%", "description": "è¡¨æ¼”è‡ªç„¶åº¦ã€ç‰©ç†é€»è¾‘çœŸå®æ„Ÿã€æ— å‡ºæˆæ„Ÿ", "scale": "1-10"},
                    "impact": {"name": "å†²å‡»åŠ› (Impact)", "weight": "20%", "description": "è§†è§‰æ˜¾è‘—æ€§ã€åŠ¨æ€å¼ åŠ›ã€ç¬¬ä¸€çœ¼å†²å‡»åŠ›", "scale": "1-10"},
                    "memorability": {"name": "è®°å¿†åº¦ (Memorability)", "weight": "20%", "description": "ç‹¬ç‰¹è§†è§‰ç¬¦å·(Von Restorffæ•ˆåº”)ã€é‡‘å¥ã€è¶£å‘³æ€§", "scale": "1-10"},
                    "fun_interest": {"name": "è¶£å‘³åº¦ (Fun/Interest)", "weight": "20%", "description": "å‚ä¸æ„Ÿã€å¨±ä¹ä»·å€¼ã€ç¤¾äº¤è´§å¸æ½œåŠ›", "scale": "1-10"}
                },
                "type_classification": {
                    "TYPE-A": "Hook/Kinetic - è§†è§‰é’©å­/é«˜èƒ½ (é«˜é¥±å’Œã€å¥‡è§‚ã€å¿«èŠ‚å¥)",
                    "TYPE-B": "Narrative/Emotion - å™äº‹/æƒ…æ„Ÿ (äººç‰©å¯¹è¯ã€ç»†å¾®è¡¨æƒ…)",
                    "TYPE-C": "Aesthetic/Vibe - æ°›å›´/ç©ºé•œ (é£æ™¯ã€æ…¢åŠ¨ä½œã€æç®€)",
                    "TYPE-D": "Commercial/Info - å•†ä¸š/å±•ç¤º (äº§å“ç‰¹å†™ã€å£æ’­)"
                },
                "selection_rules": {
                    "MUST_KEEP": "åŠ æƒæ€»åˆ† > 8.5 æˆ– ä»»æ„å•é¡¹ = 10 (æè‡´é•¿æ¿)",
                    "USABLE": "7.0 <= åŠ æƒæ€»åˆ† < 8.5 (è¿‡æ¸¡ç´ æ)",
                    "DISCARD": "åŠ æƒæ€»åˆ† < 7.0 æˆ–å­˜åœ¨è‡´å‘½ç‘•ç–µ"
                }
            },
            "scenes": [],
            "instructions": "åŸºäº Walter Murch æ³•åˆ™ï¼Œå¯¹æ¯ä¸ªåœºæ™¯è¿›è¡Œè¯„åˆ†ã€‚è€ƒè™‘åœºæ™¯ç±»å‹æƒé‡: Hookå‹ä¾§é‡IMPACT, å™äº‹å‹ä¾§é‡CREDIBILITY, æ°›å›´å‹ä¾§é‡AESTHETICS, å•†ä¸šå‹ä¾§é‡CREDIBILITY+MEMORABILITY"
        }
        for i, scene_file in enumerate(scene_files, 1):
            scene_data = {
                "scene_number": i,
                "filename": scene_file.name,
                "file_path": str(scene_file),
                "frame_path": str(self.frames_dir / f"{scene_file.stem}.jpg") if self.frames_dir.exists() else None,
                "type_classification": "TODO: é€‰æ‹© TYPE-A/B/C/D",
                "description": "TODO: ä¸€å¥è¯æè¿°ç”»é¢å†…å®¹",
                "visual_summary": "TODO: è§†è§‰å†…å®¹æ‘˜è¦",
                "scores": {"aesthetic_beauty": 0, "credibility": 0, "impact": 0, "memorability": 0, "fun_interest": 0},
                "weighted_score": 0.0,
                "selection": "TODO: [MUST KEEP] / [USABLE] / [DISCARD]",
                "selection_reasoning": "TODO: å¼•ç”¨ç›¸å…³ç†è®ºè§£é‡Šé€‰æ‹©åŸå› ",
                "edit_suggestion": "TODO: å‰ªè¾‘å»ºè®®ï¼ˆä¿ç•™å‡ ç§’ã€æ˜¯å¦éœ€è¦é™éŸ³ç­‰ï¼‰",
                "notes": "TODO: å…¶ä»–è§‚å¯Ÿç¬”è®°"
            }
            scoring_data["scenes"].append(scene_data)
        with open(self.scores_path, "w", encoding="utf-8") as f:
            json.dump(scoring_data, f, indent=2, ensure_ascii=False)
        print(f"   âœ… Scoring template created: {self.scores_path}")
        print(f"   ğŸ“ éœ€è¦å¯¹ {len(scene_files)} ä¸ªåœºæ™¯è¿›è¡Œè¯„åˆ†")
        self.results["scoring_template"] = str(self.scores_path)
        self.results["steps_completed"].append("prepare_scoring")

    def _step_ai_scene_analysis(self, scene_info: Dict):
        print("\\nğŸ¤– Step 7: Analyzing scenes with AI framework...")
        if not self.extract_scenes or scene_info["scene_count"] == 0:
            print("   âš ï¸  No scenes to analyze")
            return
        scene_files = sorted(self.scenes_dir.glob("*.mp4"))
        for i, scene_file in enumerate(scene_files, 1):
            frame_path = self.frames_dir / f"{scene_file.stem}.jpg"
            analysis = {"scene_number": i, "filename": scene_file.name, "frame_path": str(frame_path) if frame_path.exists() else None, "ai_analysis_ready": True, "notes": "è¯·æŸ¥çœ‹å¸§å›¾ç‰‡åè¿›è¡Œä¸“ä¸šåˆ†æ"}
            self.results["scene_analysis"].append(analysis)
        print(f"   âœ… å·²ä¸º {len(scene_files)} ä¸ªåœºæ™¯å‡†å¤‡ AI åˆ†ææ¡†æ¶")
        self.results["steps_completed"].append("ai_scene_analysis")

    def _step_auto_select_best_shots(self, scene_info: Dict):
        print(f"\\nâ­ Step 8: Auto-selecting best shots (threshold: {self.best_threshold})...")
        self.best_shots_dir.mkdir(exist_ok=True)
        print(f"   âœ… ç²¾é€‰ç‰‡æ®µç›®å½•å·²åˆ›å»º: {self.best_shots_dir}")
        print(f"   ğŸ“ å®Œæˆè¯„åˆ†åè¿è¡Œ: python3 scripts/scoring_helper_enhanced.py {self.scores_path} best")
        self.results["best_shots_dir"] = str(self.best_shots_dir)
        self.results["steps_completed"].append("auto_select_best_shots")

    def _step_generate_detailed_report(self, scene_info: Dict, transcript_info: Dict):
        print("\\nğŸ“„ Step 9: Generating detailed analysis report...")
        transcript_text = ""
        if self.transcript_path.exists():
            with open(self.transcript_path, 'r', encoding='utf-8') as f:
                transcript_text = f.read()
        # è¯»å–è¯¦ç»†æŠ¥å‘Šæ¨¡æ¿
        template_path = Path(__file__).parent.parent / "templates" / "detailed_report_template.md"
        with open(template_path, 'r', encoding='utf-8') as f:
            report_template = f.read()
        # æ„é€ åœºæ™¯åˆ—è¡¨è¡¨æ ¼
        scene_list_table_rows = []
        scene_files = sorted(self.scenes_dir.glob("*.mp4"))
        for i, scene_file in enumerate(scene_files, 1):
            scene_list_table_rows.append(f"| Scene {i:03d} | TODO | TODO | TODO | `{scene_file.name}` |")
        scene_list_table = "\\n".join(scene_list_table_rows)
        # æ„é€ è¯¦ç»†åœºæ™¯è¯„ä¼°éƒ¨åˆ†
        detailed_scene_evaluations_parts = []
        for i, scene_file in enumerate(scene_files, 1):
            frame_path = self.frames_dir / f"{scene_file.stem}.jpg"
            detailed_scene_evaluations_parts.append(f"""#### Scene {i:03d}: {scene_file.name}

**åŸºç¡€ä¿¡æ¯**
- **å¸§é¢„è§ˆ**: `{frame_path.name}` (è§ frames/ ç›®å½•)
- **ç‰‡æ®µæ–‡ä»¶**: `scenes/{scene_file.name}`
- **ç±»å‹åˆ†ç±»**: TODO (TYPE-A/B/C/D)

**è§†è§‰å†…å®¹æè¿°**
> TODO: è¯¦ç»†æè¿°ç”»é¢å†…å®¹ã€è¿é•œæ–¹å¼ã€ä¸»ä½“å¯¹è±¡ã€è‰²å½©æ°›å›´ç­‰

**äº”ç»´è¯„åˆ†**
| ç»´åº¦ | å¾—åˆ† | è¯„åˆ†ç†ç”± |
|------|------|---------|
| ç¾æ„Ÿ | TODO | TODO |
| å¯ä¿¡åº¦ | TODO | TODO |
| å†²å‡»åŠ› | TODO | TODO |
| è®°å¿†åº¦ | TODO | TODO |
| è¶£å‘³åº¦ | TODO | TODO |
| **åŠ æƒæ€»åˆ†** | **TODO** | - |

**ç­›é€‰å†³ç­–**
- **å»ºè®®ç­‰çº§**: TODO (MUST KEEP / USABLE / DISCARD)
- **å†³ç­–ç†ç”±**: 
  > TODO: å¼•ç”¨ç›¸å…³ç†è®ºè§£é‡Šï¼ˆå¦‚å³°ç»ˆå®šå¾‹ã€äº’è¡¥è‰²åŸç†ã€è§†è§‰æ˜¾è‘—æ€§ç­‰ï¼‰

**å‰ªè¾‘å»ºè®®**
- **ä½¿ç”¨æ–¹å¼**: TODO (å¦‚ï¼šä¿ç•™å‰3ç§’ä½œä¸ºHookï¼Œé™éŸ³ä½¿ç”¨ç­‰)
- **é€‚é…åœºæ™¯**: TODO (å¦‚ï¼šé€‚åˆä½œä¸ºå¼€åœºã€è½¬åœºã€ç»“å°¾ç­‰)

---""")
        detailed_scene_evaluations = "\\n".join(detailed_scene_evaluations_parts)
        # æ„é€ ç²¾é€‰ç‰‡æ®µè¡¨æ ¼
        best_shots_table = """| æ’å | åœºæ™¯ | åŠ æƒå¾—åˆ† | å…¥é€‰ç†ç”± | å»ºè®®ç”¨é€” |
|------|------|---------|---------|---------|
| 1 | TODO | TODO | TODO | TODO |
| 2 | TODO | TODO | TODO | TODO |
| 3 | TODO | TODO | TODO | TODO |"""
        # å¡«å……æ¨¡æ¿
        report = report_template.format(
            video_id=self.video_id,
            url=self.url,
            analysis_date=datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            video_size_mb=self.results.get('video_size_mb', 'N/A'),
            scene_count=scene_info.get('scene_count', 0),
            transcription_language=transcript_info.get('language', 'N/A'),
            transcription_segments=transcript_info.get('segment_count', 0),
            transcript_text=transcript_text[:500] if transcript_text else "(æ— è½¬å½•å†…å®¹)",
            scene_list_table=scene_list_table,
            detailed_scene_evaluations=detailed_scene_evaluations,
            best_threshold=self.best_threshold,
            best_shots_table=best_shots_table,
            video_output_dir_name=self.video_output_dir.name
        )
        with open(self.detailed_report_path, "w", encoding="utf-8") as f:
            f.write(report)
        print(f"   âœ… Detailed report generated: {self.detailed_report_path}")
        self.results["detailed_report_path"] = str(self.detailed_report_path)
        self.results["steps_completed"].append("generate_detailed_report")


def main():
    parser = argparse.ArgumentParser(
        description="Video Expert Analyzer - Enhanced Pipeline with Configurable Output",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # é¦–æ¬¡è¿è¡Œï¼Œè®¾ç½®è¾“å‡ºç›®å½•
  python3 pipeline_enhanced.py --setup

  # åˆ†æè§†é¢‘
  python3 pipeline_enhanced.py https://www.bilibili.com/video/BV1xxxxx

  # ä½¿ç”¨è‡ªå®šä¹‰è¾“å‡ºç›®å½•
  python3 pipeline_enhanced.py URL -o /path/to/output

  # ä½¿ç”¨æ›´å°çš„Whisperæ¨¡å‹
  python3 pipeline_enhanced.py URL --whisper-model tiny
        """
    )
    parser.add_argument("url", nargs="?", help="Video URL (Bilibili or YouTube)")
    parser.add_argument("-o", "--output", help="Output directory (default: from config)")
    parser.add_argument("--setup", action="store_true", help="Setup output directory configuration")
    parser.add_argument("--whisper-model", default="base", choices=["tiny", "base", "small", "medium", "large"], help="Whisper model size (default: base)")
    parser.add_argument("--scene-threshold", type=float, default=27.0, help="Scene detection threshold (default: 27.0)")
    parser.add_argument("--no-extract-scenes", action="store_true", help="Skip extracting individual scene clips")
    parser.add_argument("--best-threshold", type=float, default=7.5, help="Threshold for best shots selection (default: 7.5)")
    parser.add_argument("--json-output", help="Save results as JSON to this file")
    args = parser.parse_args()
    if args.setup:
        setup_output_directory()
        return 0
    if args.output:
        output_dir = args.output
    else:
        output_dir = get_output_directory()
        print(f"ğŸ“ ä½¿ç”¨é…ç½®ä¸­çš„è¾“å‡ºç›®å½•: {output_dir}")
    if not args.url:
        print("âŒ Error: è¯·æä¾›è§†é¢‘URLï¼Œæˆ–ä½¿ç”¨ --setup é…ç½®è¾“å‡ºç›®å½•")
        parser.print_help()
        return 1
    try:
        pipeline = VideoAnalysisPipeline(
            url=args.url,
            output_dir=output_dir,
            whisper_model=args.whisper_model,
            scene_threshold=args.scene_threshold,
            extract_scenes=not args.no_extract_scenes,
            best_threshold=args.best_threshold
        )
        results = pipeline.run()
        if args.json_output:
            with open(args.json_output, "w", encoding="utf-8") as f:
                json.dump(results, f, indent=2, ensure_ascii=False)
            print(f"\\nğŸ“Š Results saved to: {args.json_output}")
        return 0
    except Exception as e:
        print(f"\\nâŒ Error: {e}", file=sys.stderr)
        import traceback
        traceback.print_exc()
        return 1


if __name__ == "__main__":
    sys.exit(main())
